{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing Neural Machine Translation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue our quest to quantize every Neural Network!  \n",
    "On this chapter: __Google's Neural Machine Translation model__.  \n",
    "A brief summary - using stacked LSTMs and attention mechanism, this model encodes a sentence into a list of vectors and then decodes it to the other language tokens until an end token is reached.  \n",
    "To read more - refer to <a id=\"ref-1\" href=\"#cite-wu2016google\">Google's paper</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Quantizing Neural Machine Translation Models](#Quantizing-Neural-Machine-Translation-Models)\n",
    "\t* [Getting the resources](#Getting-the-resources)\n",
    "\t* [Loading the model](#Loading-the-model)\n",
    "\t* [Evaulation of the model](#Evaulation-of-the-model)\n",
    "\t* [Quantizing the model](#Quantizing-the-model)\n",
    "\t\t* [Collecting the statistics](#Collecting-the-statistics)\n",
    "\t\t* [Defining the Quantizer](#Defining-the-Quantizer)\n",
    "\t\t* [Quantizing the model](#Quantizing-the-model)\n",
    "\t\t* [Evaluating the quantized model](#Evaluating-the-quantized-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we modified the [`mlperf/training/rnn_translator`](https://github.com/mlperf/training/tree/master/rnn_translator) project to enable quantization of the GNMT model.  \n",
    "The instructions to download and setup the required environment for this task are in `README.md` (located in the current directory).  \n",
    "Download the pretrained model using the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to download the pretrained model:\n",
    "#! wget https://zenodo.org/record/2581623/files/model_best.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have everything ready to start quantizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import distiller\n",
    "from distiller.modules import DistillerLSTM\n",
    "from distiller.quantization import PostTrainLinearQuantizer\n",
    "from ast import literal_eval\n",
    "from itertools import zip_longest\n",
    "from copy import deepcopy\n",
    "\n",
    "from seq2seq import models\n",
    "from seq2seq.inference.inference import Translator\n",
    "from seq2seq.utils import AverageMeter\n",
    "import subprocess\n",
    "import os\n",
    "import seq2seq.data.config as config\n",
    "from seq2seq.data.dataset import ParallelDataset\n",
    "import logging\n",
    "from seq2seq.utils import AverageMeter\n",
    "# Import utilities from the example:\n",
    "from translate import grouper, write_output, checkpoint_from_distributed, unwrap_distributed\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logging.disable(logging.INFO)  # Disables mlperf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "batch_first=True\n",
    "batch_size=128\n",
    "beam_size=10\n",
    "cov_penalty_factor=0.1\n",
    "dataset_dir='./data'\n",
    "input='./data/newstest2014.tok.clean.bpe.32000.en'\n",
    "len_norm_const=5.0\n",
    "len_norm_factor=0.6\n",
    "max_seq_len=80\n",
    "model='model_best.pth'\n",
    "output='output_file'\n",
    "print_freq=1\n",
    "reference='./data/newstest2014.de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "checkpoint = torch.load('./model_best.pth', map_location={'cuda:0': 'cpu'})\n",
    "vocab_size = checkpoint['tokenizer'].vocab_size\n",
    "model_config = dict(vocab_size=vocab_size, math=checkpoint['config'].math,\n",
    "                    **literal_eval(checkpoint['config'].model_config))\n",
    "model_config['batch_first'] = batch_first\n",
    "model = models.GNMT(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNMT(\n",
       "  (encoder): ResidualRecurrentEncoder(\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): LSTM(1024, 1024, batch_first=True, bidirectional=True)\n",
       "      (1): LSTM(2048, 1024, batch_first=True)\n",
       "      (2): LSTM(1024, 1024, batch_first=True)\n",
       "      (3): LSTM(1024, 1024, batch_first=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (embedder): Embedding(32317, 1024, padding_idx=0)\n",
       "    (eltwiseadd_residuals): ModuleList(\n",
       "      (0): EltwiseAdd()\n",
       "      (1): EltwiseAdd()\n",
       "    )\n",
       "  )\n",
       "  (decoder): ResidualRecurrentDecoder(\n",
       "    (att_rnn): RecurrentAttention(\n",
       "      (rnn): LSTM(1024, 1024, batch_first=True)\n",
       "      (attn): BahdanauAttention(\n",
       "        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0)\n",
       "        (eltwiseadd_qk): EltwiseAdd()\n",
       "        (eltwiseadd_norm_bias): EltwiseAdd()\n",
       "        (eltwisemul_norm_scaler): EltwiseMult()\n",
       "        (tanh): Tanh()\n",
       "        (matmul_score): Matmul()\n",
       "        (softmax_att): Softmax()\n",
       "        (context_matmul): BatchMatmul()\n",
       "      )\n",
       "      (dropout): Dropout(p=0)\n",
       "    )\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): LSTM(2048, 1024, batch_first=True)\n",
       "      (1): LSTM(2048, 1024, batch_first=True)\n",
       "      (2): LSTM(2048, 1024, batch_first=True)\n",
       "    )\n",
       "    (embedder): Embedding(32317, 1024, padding_idx=0)\n",
       "    (classifier): Classifier(\n",
       "      (classifier): Linear(in_features=1024, out_features=32317, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (eltwiseadd_residuals): ModuleList(\n",
       "      (0): EltwiseAdd()\n",
       "      (1): EltwiseAdd()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = checkpoint['state_dict']\n",
    "if checkpoint_from_distributed(state_dict):\n",
    "    state_dict = unwrap_distributed(state_dict)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "torch.cuda.set_device(0)\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = checkpoint['tokenizer']\n",
    "\n",
    "\n",
    "test_data = ParallelDataset(\n",
    "    src_fname=os.path.join(dataset_dir, config.SRC_TEST_FNAME),\n",
    "    tgt_fname=os.path.join(dataset_dir, config.TGT_TEST_FNAME),\n",
    "    tokenizer=tokenizer,\n",
    "    min_len=0,\n",
    "    max_len=150,\n",
    "    sort=False)\n",
    "\n",
    "def get_loader():\n",
    "    return test_data.get_loader(batch_size=batch_size,\n",
    "                                   batch_first=True,\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=0,\n",
    "                                   drop_last=False,\n",
    "                                   distributed=False)\n",
    "def get_translator(model):\n",
    "    return Translator(model,\n",
    "                       tokenizer,\n",
    "                       beam_size=beam_size,\n",
    "                       max_seq_len=max_seq_len,\n",
    "                       len_norm_factor=len_norm_factor,\n",
    "                       len_norm_const=len_norm_const,\n",
    "                       cov_penalty_factor=cov_penalty_factor,\n",
    "                       cuda=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_path):\n",
    "    test_file = open(test_path, 'w', encoding='UTF-8')\n",
    "    model.eval()\n",
    "    translator = get_translator(model)\n",
    "    stats = {}\n",
    "    for i, (src, tgt, indices) in enumerate(tqdm(get_loader())):\n",
    "        src, src_length = src\n",
    "        if translator.batch_first:\n",
    "            batch_size = src.size(0)\n",
    "        else:\n",
    "            batch_size = src.size(1)\n",
    "        bos = [translator.insert_target_start] * (batch_size * beam_size)\n",
    "        bos = torch.LongTensor(bos)\n",
    "        if translator.batch_first:\n",
    "            bos = bos.view(-1, 1)\n",
    "        else:\n",
    "            bos = bos.view(1, -1)\n",
    "        src_length = torch.LongTensor(src_length)\n",
    "        stats['total_enc_len'] = int(src_length.sum())\n",
    "        src = src.cuda()\n",
    "        src_length = src_length.cuda()\n",
    "        bos = bos.cuda()\n",
    "        with torch.no_grad():\n",
    "            context = translator.model.encode(src, src_length)\n",
    "            context = [context, src_length, None]\n",
    "            if beam_size == 1:\n",
    "                generator = translator.generator.greedy_search\n",
    "            else:\n",
    "                generator = translator.generator.beam_search\n",
    "            preds, lengths, counter = generator(batch_size, bos, context)\n",
    "        stats['total_dec_len'] = lengths.sum().item()\n",
    "        stats['iters'] = counter\n",
    "        preds = preds.cpu()\n",
    "        lengths = lengths.cpu()\n",
    "        output = []\n",
    "        for idx, pred in enumerate(preds):\n",
    "            end = lengths[idx] - 1\n",
    "            pred = pred[1: end]\n",
    "            pred = pred.tolist()\n",
    "            out = translator.tok.detokenize(pred)\n",
    "            output.append(out)\n",
    "        output = [output[indices.index(i)] for i in range(len(output))]\n",
    "        for line in output:\n",
    "            test_file.write(line)\n",
    "            test_file.write('\\n')\n",
    "        total_tokens = stats['total_dec_len'] + stats['total_enc_len']\n",
    "    test_file.close()\n",
    "    # run moses detokenizer\n",
    "    detok_path = os.path.join(dataset_dir, config.DETOKENIZER)\n",
    "    detok_test_path = test_path + '.detok'\n",
    "\n",
    "    with open(detok_test_path, 'w') as detok_test_file, \\\n",
    "            open(test_path, 'r') as test_file:\n",
    "        subprocess.run(['perl', f'{detok_path}'], stdin=test_file,\n",
    "                       stdout=detok_test_file, stderr=subprocess.DEVNULL)\n",
    "    # run sacrebleu\n",
    "    reference_path = os.path.join(dataset_dir,\n",
    "                                  config.TGT_TEST_TARGET_FNAME)\n",
    "    sacrebleu = subprocess.run([f'sacrebleu --input {detok_test_path} '\n",
    "                                f'{reference_path} --score-only -lc --tokenize intl'],\n",
    "                               stdout=subprocess.PIPE, shell=True)\n",
    "    bleu = float(sacrebleu.stdout.strip())\n",
    "    print(f'BLEU on test dataset: {bleu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [01:32<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 22.16\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantizing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already noted, we modified the model from `mlperf` to a modular implementation so we can quantize each and every operation in the graph.  \n",
    "However, the default `nn.LSTM` was implemented in C++/CUDA, and we don't have usual access to it's operations hence we can't quantize it properly. This is why we'll convert the `nn.LSTM` to a `DistillerLSTM`, which is an entirely modular implementation of the LSTM - identical in functionality to the original `nn.LSTM`.  \n",
    "This is done by simply calling `DistillerLSTM.from_pytorch_impl` for a single `nn.LSTM` and  \n",
    "`convert_model_to_distiller_lstm` for an entire model containing multiple different LSTMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [02:51<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 22.16\n"
     ]
    }
   ],
   "source": [
    "from distiller.modules import convert_model_to_distiller_lstm\n",
    "model = convert_model_to_distiller_lstm(model)\n",
    "evaluate(model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantizer uses statistics to define the range of the quantization. We collect these statistics using a `QuantCalibrationStatsCollector` instance like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [53:57<00:00, 116.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 22.16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from distiller.data_loggers import QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "stats_file = './model_stats.yaml'\n",
    "\n",
    "if not os.path.isfile(stats_file): # Collect stats.\n",
    "    model_copy = deepcopy(model)\n",
    "    distiller.utils.assign_layer_fq_names(model_copy)\n",
    "    collector = QuantCalibrationStatsCollector(model_copy)\n",
    "    with collector_context(collector):\n",
    "        val_loss = evaluate(model_copy, output + '.temp')\n",
    "    collector.save(stats_file)\n",
    "    del model_copy\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distiller `Quantizer` object replaces each submodule in a model with its quantized counterpart, using a \n",
    "`replacement_factory`.  \n",
    "`Quantizer.replacement_factory` is a dictionary which maps from a module type (e.g. `nn.Linear` and `nn.Conv`) to a function. This function takes a module and quantization configuration, and returns a quantized version of the same module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repacing 'Conv2d' modules using 'replace_param_layer' function\n",
      "Repacing 'Linear' modules using 'replace_param_layer' function\n",
      "Repacing 'Concat' modules using 'replace_non_param_layer' function\n",
      "Repacing 'EltwiseAdd' modules using 'replace_non_param_layer' function\n",
      "Repacing 'EltwiseMult' modules using 'replace_non_param_layer' function\n",
      "Repacing 'Embedding' modules using 'replace_embedding' function\n"
     ]
    }
   ],
   "source": [
    "# We quantize everything except softmax and embedding\n",
    "overrides_yaml = \"\"\"\n",
    ".*softmax.*: \n",
    "    bits_weights: null\n",
    "    bits_activations: null\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "# Basic quantizer defintion\n",
    "quantizer = PostTrainLinearQuantizer(deepcopy(model), \n",
    "                                    mode=\"SYMMETRIC\",  # As was suggested in GNMT's paper\n",
    "                                    model_activation_stats=stats_file,\n",
    "                                    overrides=overrides)\n",
    "# We take a look at the replacement factory:\n",
    "for t, rf in quantizer.replacement_factory.items():\n",
    "    print(f\"Repacing '{t.__name__}' modules using '{rf.__name__}' function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done by simply calling `quantizer.prepare_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvds_lab/lev_z/distiller/distiller/quantization/quantizer.py:255: UserWarning: Module 'decoder.embedder' references to same module as 'encoder.embedder'. Replacing with reference the same wrapper.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNMT(\n",
       "  (encoder): ResidualRecurrentEncoder(\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): DistillerLSTM(1024, 1024, num_layers=1, dropout=0.00, bidirectional=True)\n",
       "      (1): DistillerLSTM(2048, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (2): DistillerLSTM(1024, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (3): DistillerLSTM(1024, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (embedder): RangeLinearEmbeddingWrapper(\n",
       "      (wrapped_module): Embedding(32317, 1024, padding_idx=0)\n",
       "    )\n",
       "    (eltwiseadd_residuals): ModuleList(\n",
       "      (0): RangeLinearQuantEltwiseAddWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        in_0_scale=127.00788879394531, in_0_zero_point=0.0\n",
       "        in_1_scale=127.00006103515625, in_1_zero_point=0.0\n",
       "        out_scale=63.530452728271484, out_zero_point=0.0\n",
       "        (wrapped_module): EltwiseAdd()\n",
       "      )\n",
       "      (1): RangeLinearQuantEltwiseAddWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        in_0_scale=127.00004577636719, in_0_zero_point=0.0\n",
       "        in_1_scale=63.530452728271484, in_1_zero_point=0.0\n",
       "        out_scale=42.43059158325195, out_zero_point=0.0\n",
       "        (wrapped_module): EltwiseAdd()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ResidualRecurrentDecoder(\n",
       "    (att_rnn): RecurrentAttention(\n",
       "      (rnn): DistillerLSTM(1024, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (attn): BahdanauAttention(\n",
       "        (linear_q): RangeLinearQuantParamLayerWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          w_scale=53.0649, w_zero_point=0.0000\n",
       "          in_scale=127.0000, in_zero_point=0.0000\n",
       "          out_scale=4.2200, out_zero_point=0.0000\n",
       "          (wrapped_module): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (linear_k): RangeLinearQuantParamLayerWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          w_scale=40.9795, w_zero_point=0.0000\n",
       "          in_scale=42.4306, in_zero_point=0.0000\n",
       "          out_scale=4.5466, out_zero_point=0.0000\n",
       "          (wrapped_module): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0)\n",
       "        (eltwiseadd_qk): RangeLinearQuantEltwiseAddWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          in_0_scale=4.219996452331543, in_0_zero_point=0.0\n",
       "          in_1_scale=4.546571731567383, in_1_zero_point=0.0\n",
       "          out_scale=2.974982261657715, out_zero_point=0.0\n",
       "          (wrapped_module): EltwiseAdd()\n",
       "        )\n",
       "        (eltwiseadd_norm_bias): RangeLinearQuantEltwiseAddWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          in_0_scale=2.974982261657715, in_0_zero_point=0.0\n",
       "          in_1_scale=109.55178833007812, in_1_zero_point=0.0\n",
       "          out_scale=2.961179256439209, out_zero_point=0.0\n",
       "          (wrapped_module): EltwiseAdd()\n",
       "        )\n",
       "        (eltwisemul_norm_scaler): RangeLinearQuantEltwiseMultWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          in_0_scale=771.8616333007812, in_0_zero_point=0.0\n",
       "          in_1_scale=100.56507873535156, in_1_zero_point=0.0\n",
       "          out_scale=611.1993408203125, out_zero_point=0.0\n",
       "          (wrapped_module): EltwiseMult()\n",
       "        )\n",
       "        (tanh): Tanh()\n",
       "        (matmul_score): Matmul()\n",
       "        (softmax_att): Softmax()\n",
       "        (context_matmul): BatchMatmul()\n",
       "      )\n",
       "      (dropout): Dropout(p=0)\n",
       "    )\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): DistillerLSTM(2048, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (1): DistillerLSTM(2048, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (2): DistillerLSTM(2048, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "    )\n",
       "    (embedder): RangeLinearEmbeddingWrapper(\n",
       "      (wrapped_module): Embedding(32317, 1024, padding_idx=0)\n",
       "    )\n",
       "    (classifier): Classifier(\n",
       "      (classifier): RangeLinearQuantParamLayerWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        w_scale=40.8311, w_zero_point=0.0000\n",
       "        in_scale=42.8144, in_zero_point=0.0000\n",
       "        out_scale=6.3242, out_zero_point=0.0000\n",
       "        (wrapped_module): Linear(in_features=1024, out_features=32317, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (eltwiseadd_residuals): ModuleList(\n",
       "      (0): RangeLinearQuantEltwiseAddWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        in_0_scale=127.01639556884766, in_0_zero_point=0.0\n",
       "        in_1_scale=127.00018310546875, in_1_zero_point=0.0\n",
       "        out_scale=63.68953323364258, out_zero_point=0.0\n",
       "        (wrapped_module): EltwiseAdd()\n",
       "      )\n",
       "      (1): RangeLinearQuantEltwiseAddWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        in_0_scale=127.00001525878906, in_0_zero_point=0.0\n",
       "        in_1_scale=63.68953323364258, in_1_zero_point=0.0\n",
       "        out_scale=42.81440353393555, out_zero_point=0.0\n",
       "        (wrapped_module): EltwiseAdd()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.prepare_model()\n",
    "quantizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to know how these functions replace the modules - I recommend reading the source code for them in  \n",
    "`{DISTILLER_ROOT}/distiller/quantization/range_linear.py:PostTrainLinearQuantizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [13:33<00:00, 28.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 17.79\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "evaluate(quantizer.model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, we quantized our model entirely - but it retained its accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a id=\"cite-wu2016google\"/><sup><a href=#ref-1>[^]</a></sup>Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others. 2016. _Google's neural machine translation system: Bridging the gap between human and machine translation_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--bibtex\n",
    "\n",
    "@article{wu2016google,\n",
    "  title={Google's neural machine translation system: Bridging the gap between human and machine translation},\n",
    "  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},\n",
    "  journal={arXiv preprint arXiv:1609.08144},\n",
    "  year={2016}\n",
    "}\n",
    "\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
